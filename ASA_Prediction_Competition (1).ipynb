{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2026 ASA Prediction Competition: HDL Cholesterol Prediction\n",
        "**Team Members:** Matheus Gomes, Satvik Hulikere, Joaquin Hidalgo, Caio Rocha  \n",
        "**Objective:** To develop a robust machine learning pipeline to predict Direct HDL-Cholesterol levels using dietary and demographic data from the NHANES dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "BzDruu3RQrr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBFCj214iKZL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "LpCQfTzviNM3",
        "outputId": "d6f012e0-c6bc-4ea7-92d5-6f0fd92f1654"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2521535812.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Reading CSV training file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Reading CSV training file\n",
        "df = pd.read_csv(\"/train.csv\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMLIjE0OinPg"
      },
      "outputs": [],
      "source": [
        "# Checking missing values\n",
        "df.info()\n",
        "\n",
        "# Checking duplicate values\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ› ï¸ Data Preprocessing & Feature Engineering\n",
        "To improve model performance and interpretability, we perform the following steps:\n",
        "* **Renaming:** Converting technical NHANES codes (e.g., `DR1TKCAL`) into human-readable labels using the provided variable mapping.\n",
        "* **Feature Removal:** We drop administrative metadata (Interviewer IDs, Sample Weights) and logistical variables that do not carry physiological significance for cholesterol prediction.\n",
        "* **Duplicate Handling:** Ensuring data integrity by removing any redundant entries."
      ],
      "metadata": {
        "id": "aVw_vhmyQ1wb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQLJBHzunuQr"
      },
      "outputs": [],
      "source": [
        "# List of columns to be removed for now (id or interview info variables)\n",
        "cols_to_drop = [\n",
        "    'Unnamed: 0', 'DR1MRESP', 'DR1HELP',\n",
        "    'DR1LANG', 'DR1EXMER'\n",
        "]\n",
        "\n",
        "# Dropping the columns directly from the main dataframe\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Verify the result\n",
        "print(f\"Current number of columns: {len(df.columns)}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXp8diRkpye-"
      },
      "outputs": [],
      "source": [
        "# 1. Load the variable labels\n",
        "labels_df = pd.read_csv(\"/variable_labels.csv\")\n",
        "\n",
        "# 2. Create a dictionary mapping: {'Variable_Name': 'Human_Readable_Label'}\n",
        "# This assumes the CSV has 'variable' and 'label' columns\n",
        "label_dict = dict(zip(labels_df['variable'], labels_df['label']))\n",
        "\n",
        "# 3. Rename the columns in your main dataframe\n",
        "df.rename(columns=label_dict, inplace=True)\n",
        "\n",
        "# 4. View the result with the new readable names\n",
        "print(\"Columns renamed successfully!\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Feature Selection: Variance & Multicollinearity\n",
        "We apply statistical filters to streamline the feature set:\n",
        "1. **Low Variance Filter:** Removing features with near-zero variance that offer no predictive power.\n",
        "2. **Correlation Analysis:** Identifying highly correlated features ($r > 0.90$). We remove redundant variables (like specific Fatty Acid sub-types) to prevent **Multicollinearity**, which can destabilize linear model coefficients."
      ],
      "metadata": {
        "id": "kjpZY0ekQ8gh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pixEh-p7aTUc"
      },
      "source": [
        "START FILTERING UNNECESSARY COLUMNS/VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZulSJVQaV-T"
      },
      "outputs": [],
      "source": [
        "# sample variance from each column\n",
        "variance = df.var()\n",
        "\n",
        "# store low variance variables in a list\n",
        "low_variance_cols = [col for col, var in variance.items() if var < 0.005]\n",
        "\n",
        "# identify variables to be removed based on their variance\n",
        "print(\"Columns with low variance: \", low_variance_cols)\n",
        "\n",
        "# look further into these variables\n",
        "df[['PFA 18:4 (Octadecatetraenoic) (gm)', 'Breast-fed infant (either day)', 'Ever had a drink of any kind of alcohol']].head(10)\n",
        "\n",
        "# print correlation from each low variance column against the target\n",
        "low_var_corr = [df[col].corr(df['Direct HDL-Cholesterol (mg/dL)'])\n",
        " for col in low_variance_cols]\n",
        "print(low_var_corr)\n",
        "\n",
        "# drop low variance and uncorrelated columns\n",
        "df = df.drop(columns=['Breast-fed infant (either day)', 'Ever had a drink of any kind of alcohol'])\n",
        "\n",
        "print(f\"Current number of columns: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mam-QI4ke82"
      },
      "source": [
        "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
        "  c /= stddev[:, None]\n",
        "\n",
        "  This tells us standard deviation is 0. Remove these columns. Keep 'PFA 18:4 (Octadecatetraenoic) (gm)' despite low linear correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8JhtNOLrpQN"
      },
      "source": [
        "perform feature to feature correlation analysis. Remove highly correlated features to prevent multicollinearity later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gJdXxfGkmqT"
      },
      "outputs": [],
      "source": [
        "# keep removing based on feature to feature correlation\n",
        "\n",
        "# separate features and target\n",
        "target_col = 'Direct HDL-Cholesterol (mg/dL)'\n",
        "\n",
        "y = df[target_col]\n",
        "X = df.drop(columns=[target_col])\n",
        "\n",
        "# obtain correlation among predictors only\n",
        "corr_matrix = X.corr().abs()\n",
        "\n",
        "# only obtain upper triangle to avoid duplicate comparisons\n",
        "upper = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# identify highly correlated features (>0.9)\n",
        "to_drop = [column for column in upper.columns\n",
        "           if any(upper[column] > 0.9)]\n",
        "print(to_drop)\n",
        "\n",
        "# drop highly correlated features\n",
        "df = df.drop(columns=to_drop)\n",
        "\n",
        "print(f\"Current number of columns: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXfkzH3Ns1O1"
      },
      "source": [
        "We will not be removing variables based on weak correlation against the target variable due to that they show linear relationship. However, we will be using regularized linear regression for prediction as well as tree based models which dont rely on this. We will use this kind of models due to the high amount of variables we have, we must avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsrBXHvwRmO"
      },
      "source": [
        "STEPS TO FOLLOW:\n",
        "\n",
        "5ï¸âƒ£ Baseline Model\n",
        "\n",
        "Train a simple model first:\n",
        "\n",
        "Linear Option\n",
        "\n",
        "LASSO, Ridge, or ElasticNet\n",
        "\n",
        "Tree Option\n",
        "\n",
        "Random Forest or XGBoost\n",
        "\n",
        "Evaluate performance (RMSE, RÂ²) on validation set.\n",
        "\n",
        "6ï¸âƒ£ Feature Importance / Refinement\n",
        "\n",
        "For tree models, check feature_importances_\n",
        "\n",
        "Remove weak features or try feature grouping if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtJo3DfywS-m"
      },
      "outputs": [],
      "source": [
        "# check for outliers and plausible values\n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMWR_b0Lc6S_"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ¤– Model Development & Evaluation\n",
        "We evaluate multiple algorithms to balance simplicity and predictive power. Our primary metric is **Root Mean Squared Error (RMSE)**.\n",
        "* **Lasso Regression:** Used as a baseline to perform automated feature selection through L1 regularization.\n",
        "* **Random Forest:** A non-linear ensemble approach to capture complex interactions.\n",
        "* **XGBoost:** Our final optimized model, utilizing gradient boosting with tuned hyperparameters to minimize the prediction gap."
      ],
      "metadata": {
        "id": "dmwoCuYcRGVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyNvHDKbeuPx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Define Features and Target\n",
        "X = df.drop(columns=['Direct HDL-Cholesterol (mg/dL)'])\n",
        "y = df['Direct HDL-Cholesterol (mg/dL)']\n",
        "\n",
        "# 2. Split into Training and Validation sets (80/20)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Scale the data (Crucial for Lasso!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgKuv8Ukh7k-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# 1. Initialize LassoCV\n",
        "# We provide a range of alphas to test (from 0.001 to 10)\n",
        "alphas = np.logspace(-2, 2, 100)\n",
        "lasso_cv = LassoCV(alphas=alphas, cv=10, max_iter=20000, random_state=42)\n",
        "\n",
        "# 2. Fit the model on scaled training data\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Best Alpha found\n",
        "print(f\"Optimal Alpha: {lasso_cv.alpha_:.6f}\")\n",
        "\n",
        "# 4. Evaluate on validation set\n",
        "# Training predictions\n",
        "y_train_pred = lasso_cv.predict(X_train_scaled)\n",
        "\n",
        "# Training metrics\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "y_cv_pred = lasso_cv.predict(X_val_scaled)\n",
        "\n",
        "cv_rmse = np.sqrt(mean_squared_error(y_val, y_cv_pred))\n",
        "cv_r2 = r2_score(y_val, y_cv_pred)\n",
        "\n",
        "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Training RÂ²: {train_r2:.4f}\")\n",
        "\n",
        "# Validation metrics (already computed)\n",
        "print(f\"Validation RMSE: {cv_rmse:.4f}\")\n",
        "print(f\"Validation RÂ²: {cv_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9qtAz0o9EP"
      },
      "source": [
        "Optimal Alpha: 0.236449\n",
        "Training RMSE: 5.4001\n",
        "Training RÂ²: 0.6362\n",
        "Validation RMSE: 5.9078\n",
        "Validation RÂ²: 0.5813\n",
        "\n",
        "Model including highly correlated features slightly outperformed old model. Keep all features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXc0P9KQryYe"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42) # we shuffle for cv to be unbiased\n",
        "\n",
        "# Alpha grid (Ridge typically prefers slightly larger values than Lasso)\n",
        "alphas = np.logspace(-2, 2, 100)\n",
        "\n",
        "# Initialize RidgeCV\n",
        "ridge_cv = RidgeCV(\n",
        "    alphas=alphas,\n",
        "    cv=kf\n",
        ")\n",
        "\n",
        "# Fit on scaled training data\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best alpha\n",
        "print(f\"Optimal Alpha: {ridge_cv.alpha_:.6f}\")\n",
        "\n",
        "# ---- Training predictions ----\n",
        "y_train_pred = ridge_cv.predict(X_train_scaled)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# ---- Validation predictions ----\n",
        "y_cv_pred = ridge_cv.predict(X_val_scaled)\n",
        "\n",
        "cv_rmse = np.sqrt(mean_squared_error(y_val, y_cv_pred))\n",
        "cv_r2 = r2_score(y_val, y_cv_pred)\n",
        "\n",
        "# ---- Print results ----\n",
        "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Training RÂ²: {train_r2:.4f}\")\n",
        "\n",
        "print(f\"Validation RMSE: {cv_rmse:.4f}\")\n",
        "print(f\"Validation RÂ²: {cv_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTJJ9QcZuBji"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "# L1 ratio grid: 0=L2 (Ridge), 1=L1 (Lasso), 0.5=balanced\n",
        "l1_ratios = [0.9, 0.95, 0.98, 0.99] # lean more to lasso side since its performing better\n",
        "\n",
        "# Initialize ElasticNetCV\n",
        "elastic_cv = ElasticNetCV(\n",
        "    alphas=alphas,\n",
        "    l1_ratio=l1_ratios,\n",
        "    cv=10,\n",
        "    max_iter=50000,\n",
        "    tol=1e-3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit model on scaled training data\n",
        "elastic_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best alpha & l1_ratio\n",
        "print(f\"Optimal Alpha: {elastic_cv.alpha_:.6f}\")\n",
        "print(f\"Optimal L1 Ratio: {elastic_cv.l1_ratio_:.2f}\")\n",
        "\n",
        "# ---- Training predictions ----\n",
        "y_train_pred = elastic_cv.predict(X_train_scaled)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# ---- Validation predictions ----\n",
        "y_cv_pred = elastic_cv.predict(X_val_scaled)\n",
        "cv_rmse = np.sqrt(mean_squared_error(y_val, y_cv_pred))\n",
        "cv_r2 = r2_score(y_val, y_cv_pred)\n",
        "\n",
        "# ---- Print results ----\n",
        "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Training RÂ²: {train_r2:.4f}\")\n",
        "print(f\"Validation RMSE: {cv_rmse:.4f}\")\n",
        "print(f\"Validation RÂ²: {cv_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_M7vOLEvhEZ"
      },
      "source": [
        "Conlcusion: Lasso is the better choice at this point\n",
        "\n",
        "check for distributions as a last resort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wWBI0uUwoFn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Histogram of target\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(y_train, bins=30, kde=True, color='skyblue')\n",
        "plt.title(\"Target Distribution - Histogram\")\n",
        "plt.xlabel(\"y_train\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# Boxplot of target\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=y_train, color='lightgreen')\n",
        "plt.title(\"Target Distribution - Boxplot\")\n",
        "plt.xlabel(\"y_train\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dusEp7bJlrJ2"
      },
      "source": [
        "We will not be adding interactions in the linear models to keep them simple. We will explore non linear relationships later with tree based models. Afterwards, we are considering stacking the two type of models to get model diversity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yHW5Q_mtHS7"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSAKoH9RuFlC"
      },
      "outputs": [],
      "source": [
        "'''from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define a focused search space\n",
        "param_dist = {\n",
        "    'n_estimators': [200, 300, 500],\n",
        "    'max_depth': [8, 10, 12],                # Testing around your current 10\n",
        "    'min_samples_split': [5, 10, 15],        # Higher helps reduce overfitting\n",
        "    'min_samples_leaf': [2, 4, 6],           # Key for smoothing out predictions\n",
        "    'max_features': ['sqrt', 'log2', 0.3],   # Limits features per tree (vital for 77 features)\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "\n",
        "rf_search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=10,                                   # Use 10-fold as we discussed\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train on ORIGINAL features\n",
        "rf_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best RF Params: {rf_search.best_params_}\")\n",
        "print(f\"Best CV RMSE: {-rf_search.best_score_:.4f}\") '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ia3YQF0tMpx"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# 1. Initialize the Random Forest with the optimized parameters from your CV\n",
        "rf_model_optimized = RandomForestRegressor(\n",
        "    n_estimators=600,      # Increased from 100 based on CV\n",
        "    max_depth=10,          # Added based on CV to prevent overfitting\n",
        "    min_samples_split=5,  # Added based on CV for better generalization\n",
        "    min_samples_leaf=2,\n",
        "    max_features=0.5,     # CV found that looking at all 77 features worked best\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 2. Fit the model using your scaled training data\n",
        "rf_model_optimized.fit(X_train, y_train)\n",
        "\n",
        "# 3. Make predictions on the validation set\n",
        "y_rf_pred = rf_model_optimized.predict(X_val)\n",
        "\n",
        "# 4. Evaluate Performance\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_val, y_rf_pred))\n",
        "rf_r2 = r2_score(y_val, y_rf_pred)\n",
        "\n",
        "print(f\"Optimized Random Forest RMSE: {rf_rmse:.4f}\")\n",
        "print(f\"Optimized Random Forest RÂ²: {rf_r2:.4f}\")\n",
        "\n",
        "# 1. Get predictions for both sets (using SCALED data as per your RF training)\n",
        "y_rf_train_pred = rf_model_optimized.predict(X_train)\n",
        "y_rf_val_pred = rf_model_optimized.predict(X_val)\n",
        "\n",
        "# 2. Calculate RMSE\n",
        "rf_train_rmse = np.sqrt(mean_squared_error(y_train, y_rf_train_pred))\n",
        "rf_val_rmse = np.sqrt(mean_squared_error(y_val, y_rf_val_pred))\n",
        "\n",
        "print(f\"--- Random Forest Performance ---\")\n",
        "print(f\"Train RMSE: {rf_train_rmse:.4f}\")\n",
        "print(f\"Validation RMSE: {rf_val_rmse:.4f}\")\n",
        "print(f\"Difference (Gap): {rf_val_rmse - rf_train_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9cyfv2o_Sl7"
      },
      "source": [
        "Best performance with optimal parameters:\n",
        "\n",
        "Optimized Random Forest RMSE: 5.1240\n",
        "Optimized Random Forest RÂ²: 0.6850\n",
        "--- Random Forest Performance ---\n",
        "Train RMSE: 2.1833\n",
        "Validation RMSE: 5.1240\n",
        "Difference (Gap): 2.9407\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMU76UzEp-1f"
      },
      "source": [
        "Looks like tree is slightly overfitted. Find optimal parameters by testing:\n",
        "- tree depth\n",
        "- leave size\n",
        "- features consdidered each split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQey-XlYmpg3"
      },
      "outputs": [],
      "source": [
        "'''# Find optimal parameters for RF\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Base model\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Parameter grid\n",
        "param_dist = {\n",
        "    \"n_estimators\": [400],\n",
        "    \"max_depth\": [6, 8, 10],   # dont let tree get too deep to avoid overfitting\n",
        "    \"min_samples_split\": [6, 8, 10],\n",
        "    \"min_samples_leaf\": [3, 6, 9],\n",
        "    \"max_features\": [\"sqrt\", 0.5]\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "rf_random = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=40,               # number of random combinations\n",
        "    cv=5,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# fit model on training data\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", rf_random.best_params_)\n",
        "print(\"Best CV RMSE:\", -rf_random.best_score_)\n",
        "\n",
        "# Use optimal parameters\n",
        "rf_best = rf_random.best_estimator_\n",
        "\n",
        "y_train_pred = rf_best.predict(X_train)\n",
        "y_val_pred = rf_best.predict(X_val)\n",
        "\n",
        "# Validate model on training and testing data (RMSE)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "# (R^2)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "print(f\"Gap: {val_rmse - train_rmse:.4f}\")\n",
        "print(f\"Train RÂ²: {train_r2:.4f}\")\n",
        "print(f\"Validation RÂ²: {val_r2:.4f}\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "403VV57Utuai"
      },
      "source": [
        "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
        "Best Parameters: {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.5, 'max_depth': 10}\n",
        "Best CV RMSE: 4.826052255457246\n",
        "Train RMSE: 2.1911\n",
        "Validation RMSE: 5.1336\n",
        "Gap: 2.9425\n",
        "Train RÂ²: 0.9401\n",
        "Validation RÂ²: 0.6839\n",
        "\n",
        "optimal parameters for now. test a few more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1qmDJwqzkDz"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx7eKp4FSZF-"
      },
      "source": [
        "now we run it again, but this time in crease the reg_lambda which forces the model to dsitrubute importance more effecitvely rather than over-relying on one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrcgFjz67_3_"
      },
      "outputs": [],
      "source": [
        "'''import xgboost as xgb\n",
        "\n",
        "# Use the best parameters found earlier, but increase regularization\n",
        "xgb_lean = xgb.XGBRegressor(\n",
        "    n_estimators=1000,      # Increase trees...\n",
        "    learning_rate=0.01,     # ...but slow down learning for precision\n",
        "    max_depth=5,            # Keep it shallow to prevent overfitting\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=20,          # Increase L2 to handle the BMI/Waist correlation\n",
        "    reg_alpha=1,            # Added L1 to further prune remaining noise\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit with Early Stopping to find the exact moment the model peaks\n",
        "xgb_lean.fit(\n",
        "    X_train_lean, y_train,\n",
        "    eval_set=[(X_val_lean, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_lean_pred = xgb_lean.predict(X_val_lean)\n",
        "lean_rmse = np.sqrt(mean_squared_error(y_val, y_lean_pred))\n",
        "lean_r2 = r2_score(y_val, y_lean_pred)\n",
        "\n",
        "print(f\"Lean XGBoost RMSE: {lean_rmse:.4f}\")\n",
        "print(f\"Lean XGBoost RÂ²: {lean_r2:.4f}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nuA5dxsD7KO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# test parameters for xgboost to find best performance\n",
        "import xgboost as xgb\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "param_dist = {\n",
        "    'learning_rate': uniform(0.01, 0.09),      # 0.01â€“0.1\n",
        "    'max_depth': randint(3, 8),               # 3â€“7\n",
        "    'min_child_weight': randint(1, 6),        # 1â€“5\n",
        "    'subsample': uniform(0.6, 0.3),           # 0.6â€“0.9\n",
        "    'colsample_bytree': uniform(0.6, 0.3),    # 0.6â€“0.9\n",
        "    'gamma': uniform(0, 5),                   # 0â€“5\n",
        "    'reg_alpha': uniform(0, 5),               # 0â€“5\n",
        "    'reg_lambda': uniform(1, 10),             # 1â€“11\n",
        "}\n",
        "\n",
        "xgb_random = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,              # number of random combinations\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_random.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", xgb_random.best_params_)\n",
        "print(\"Best CV RMSE:\", -xgb_random.best_score_)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlnJ_bcmP6-d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# 4. Use the best parameters and add early stopping\n",
        "best_params = xgb_random.best_params_\n",
        "\n",
        "best_xgb = xgb.XGBRegressor(\n",
        "    n_estimators=10000,      # large number, early stopping will pick the optimal\n",
        "    **best_params,           # unpack all the tuned parameters\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "# 7ï¸âƒ£ Predictions\n",
        "y_train_pred = best_xgb.predict(X_train)\n",
        "y_val_pred = best_xgb.predict(X_val)\n",
        "\n",
        "# 8ï¸âƒ£ Evaluate performance\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Train RMSE: {train_rmse:.4f}, RÂ²: {train_r2:.4f}\")\n",
        "print(f\"Val RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}\")\n",
        "print(f\"RMSE Gap (Val - Train): {val_rmse - train_rmse:.4f}\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3HKN8kBUJXj"
      },
      "source": [
        "Regular Cross-Validated xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bfq_lxXzmR-"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# 1. Initialize the XGBoost Regressor\n",
        "# We'll start with some robust default 'learning_rate' and 'max_depth'\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,     # More trees than RF, but they are 'shallower'\n",
        "    learning_rate=0.01,   # 'eta' - smaller steps prevent overfitting\n",
        "    max_depth=3,          # Typical range 3-10\n",
        "    subsample=0.8,        # Use 80% of data for each tree to add randomness\n",
        "    colsample_bytree=0.8, # Similar to max_features in RF\n",
        "    random_state=42,\n",
        "    reg_lambda = 10,\n",
        "    reg_alpha = 1,\n",
        "    objective='reg:squarederror' # Explicitly set objective for clarity\n",
        ")\n",
        "\n",
        "# Define the stopping logic separately\n",
        "early_stop = xgb.callback.EarlyStopping(\n",
        "    rounds=50,\n",
        "    metric_name='rmse',\n",
        "    data_name='validation_0' # Refers to the first set in eval_set\n",
        ")\n",
        "\n",
        "# Now your .fit() looks almost like it used to\n",
        "xgb_model.fit(\n",
        "    X_train, y_train,\n",
        " # This replaces the old early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# 3. Make predictions\n",
        "y_xgb_pred = xgb_model.predict(X_val)\n",
        "\n",
        "# 4. Evaluate Performance\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_val, y_xgb_pred))\n",
        "xgb_r2 = r2_score(y_val, y_xgb_pred)\n",
        "\n",
        "print(f\"XGBoost RMSE: {xgb_rmse:.4f}\")\n",
        "print(f\"XGBoost RÂ²: {xgb_r2:.4f}\")\n",
        "\n",
        "# 1. Get predictions for both sets\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "\n",
        "# 2. Calculate RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f\"--- XGBoost Performance ---\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "print(f\"Difference (Gap): {val_rmse - train_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nlq34vQ0w2Y"
      },
      "outputs": [],
      "source": [
        "'''from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define the search space\n",
        "param_dist = {\n",
        "    'n_estimators': [500, 1000, 2000],          # High number, let early stopping handle it\n",
        "    'learning_rate': [0.01, 0.05, 0.1],         # Smaller is usually better but slower\n",
        "    'max_depth': [3, 5, 7, 9],                  # Start shallow (3-5) to avoid overfit\n",
        "    'min_child_weight': [1, 5, 10],             # Higher values prevent small, noisy leaves\n",
        "    'subsample': [0.7, 0.8, 0.9],               # Row sampling\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8],        # Feature sampling (crucial for 77 features)\n",
        "    'reg_alpha': [0, 0.1, 1, 10],               # L1 (Lasso-style)\n",
        "    'reg_lambda': [1, 5, 10]                    # L2 (Ridge-style)\n",
        "}\n",
        "\n",
        "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Run Random Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,             # Number of random combinations to try\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Remember: No scaling needed for XGBoost!\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Params: {random_search.best_params_}\")\n",
        "print(f\"Best Score: {-random_search.best_score_}\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzLFtZOv3TMe"
      },
      "source": [
        "Stacking (Random Forest + Lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRp_gtAs3d5o"
      },
      "outputs": [],
      "source": [
        "# Use split data from before\n",
        "\n",
        "lasso = Lasso(alpha=0.1, max_iter = 10000)\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_predict\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "lasso_oof = cross_val_predict(lasso, X_train, y_train, cv=kf)\n",
        "rf_oof = cross_val_predict(rf, X_train, y_train, cv=kf)\n",
        "\n",
        "# Stack predictions as new features\n",
        "X_meta_train = np.column_stack([lasso_oof, rf_oof])\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "meta_model.fit(X_meta_train, y_train)\n",
        "\n",
        "# Fit base models on full training data\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "lasso_val_pred = lasso.predict(X_val_scaled)\n",
        "rf_val_pred = rf.predict(X_val)\n",
        "\n",
        "X_meta_val = np.column_stack([lasso_val_pred, rf_val_pred])\n",
        "\n",
        "# Final stacked predictions\n",
        "stack_val_pred = meta_model.predict(X_meta_val)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlC9hF-A7I_-"
      },
      "outputs": [],
      "source": [
        "def eval_reg(y_true, y_pred, name):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{name:>10} | RMSE = {rmse:.4f} | R^2 = {r2:.4f}\")\n",
        "\n",
        "eval_reg(y_val, stack_val_pred, \"Stacked\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVrQb42rWRRG"
      },
      "source": [
        "Stacking with Lasso, random forest, and xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuxaurovWcM5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the base models using your previous best parameters\n",
        "base_models = [\n",
        "    ('lasso', lasso_cv), # Uses the optimized alpha you found\n",
        "    ('rf', rf_model_optimized), # The 200-tree model from cell 89\n",
        "    ('xgb', xgb_model) # The optimized XGBoost from your search\n",
        "]\n",
        "\n",
        "# 2. Initialize the Stacking Regressor\n",
        "# The final_estimator 'Ridge' will decide how much to weight each model's prediction\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=Ridge(alpha=1.0),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 3. Fit the stack\n",
        "# This will take a moment as it does cross-validation internally\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and Evaluate\n",
        "y_stack_pred = stacking_model.predict(X_val)\n",
        "stack_rmse = np.sqrt(mean_squared_error(y_val, y_stack_pred))\n",
        "stack_r2 = r2_score(y_val, y_stack_pred)\n",
        "\n",
        "print(f\"Stacking Model RMSE: {stack_rmse:.4f}\")\n",
        "print(f\"Stacking Model RÂ²: {stack_r2:.4f}\")\n",
        "\n",
        "\n",
        "# 1. Generate predictions for both sets using the Triple Stack\n",
        "# This uses the model from Cell 39\n",
        "y_stack_train_pred = stacking_model.predict(X_train)\n",
        "y_stack_val_pred = stacking_model.predict(X_val)\n",
        "\n",
        "# 2. Calculate RMSE\n",
        "stack_train_rmse = np.sqrt(mean_squared_error(y_train, y_stack_train_pred))\n",
        "stack_val_rmse = np.sqrt(mean_squared_error(y_val, y_stack_val_pred))\n",
        "\n",
        "# 3. Calculate RÂ² for the full picture\n",
        "stack_train_r2 = r2_score(y_train, y_stack_train_pred)\n",
        "stack_val_r2 = r2_score(y_val, y_stack_val_pred)\n",
        "\n",
        "print(f\"--- Triple Stack (Lasso + RF + XGB) Performance ---\")\n",
        "print(f\"Train RMSE: {stack_train_rmse:.4f} | Train RÂ²: {stack_train_r2:.4f}\")\n",
        "print(f\"Validation RMSE: {stack_val_rmse:.4f} | Validation RÂ²: {stack_val_r2:.4f}\")\n",
        "print(f\"Gap (Overfitting): {stack_val_rmse - stack_train_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpLZT3db2f-"
      },
      "source": [
        "Stacking Lasso and xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beK_ZpwTb8ya"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# 1. Define the base models (Lasso and XGBoost only)\n",
        "base_models_lean = [\n",
        "    ('lasso', lasso_cv),\n",
        "    ('xgb', xgb_model)\n",
        "]\n",
        "\n",
        "# 2. Initialize the Stacking Regressor\n",
        "# We keep Ridge as the final estimator to prevent the meta-model from overfitting\n",
        "stack_lasso_xgb = StackingRegressor(\n",
        "    estimators=base_models_lean,\n",
        "    final_estimator=Ridge(alpha=1.0),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 3. Fit the model\n",
        "stack_lasso_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. Predict and Evaluate\n",
        "y_stack_lean_pred = stack_lasso_xgb.predict(X_val_scaled)\n",
        "stack_lean_rmse = np.sqrt(mean_squared_error(y_val, y_stack_lean_pred))\n",
        "stack_lean_r2 = r2_score(y_val, y_stack_lean_pred)\n",
        "\n",
        "print(f\"Lasso + XGBoost Stack RMSE: {stack_lean_rmse:.4f}\")\n",
        "print(f\"Lasso + XGBoost Stack RÂ²: {stack_lean_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwE5g-BNcqDA"
      },
      "source": [
        "Stacking xgboost and randomforest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvCgxhBVcvZ4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the base models (RF and XGBoost only)\n",
        "base_models_trees = [\n",
        "    ('rf', rf_model_optimized),\n",
        "    ('xgb', xgb_model)\n",
        "]\n",
        "\n",
        "# 2. Initialize the Stacking Regressor\n",
        "stack_trees = StackingRegressor(\n",
        "    estimators=base_models_trees,\n",
        "    final_estimator=Ridge(alpha=1.0),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 3. Fit the model\n",
        "stack_trees.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and Evaluate\n",
        "y_stack_trees_pred = stack_trees.predict(X_val)\n",
        "stack_trees_rmse = np.sqrt(mean_squared_error(y_val, y_stack_trees_pred))\n",
        "stack_trees_r2 = r2_score(y_val, y_stack_trees_pred)\n",
        "\n",
        "print(f\"RF + XGBoost Stack RMSE: {stack_trees_rmse:.4f}\")\n",
        "print(f\"RF + XGBoost Stack RÂ²: {stack_trees_r2:.4f}\")\n",
        "\n",
        "# 1. Generate predictions for both sets\n",
        "# stack_trees is your StackingRegressor from Cell 54\n",
        "y_stack_train_pred = stack_trees.predict(X_train)\n",
        "y_stack_val_pred = stack_trees.predict(X_val)\n",
        "\n",
        "# 2. Calculate RMSE\n",
        "stack_train_rmse = np.sqrt(mean_squared_error(y_train, y_stack_train_pred))\n",
        "stack_val_rmse = np.sqrt(mean_squared_error(y_val, y_stack_val_pred))\n",
        "\n",
        "print(f\"--- Stacking (RF + XGBoost) Performance ---\")\n",
        "print(f\"Train RMSE: {stack_train_rmse:.4f}\")\n",
        "print(f\"Validation RMSE: {stack_val_rmse:.4f}\")\n",
        "print(f\"Gap: {stack_val_rmse - stack_train_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSEiIpWDdtuu"
      },
      "source": [
        "Model Performance Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SlojAHhdyWY"
      },
      "source": [
        "Model Combination,RMSE (Lower is Better),RÂ² (Higher is Better)\n",
        "RF + XGBoost (Winner),5.0156,0.6982\n",
        "Lasso + Random Forest,5.0436,0.6948\n",
        "Lasso + RF + XGBoost,5.0465,0.6945\n",
        "Lasso + XGBoost,5.0470,0.6944"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P96Rlp3uN5_9"
      },
      "source": [
        "Actual Vs Predicted Graph for Lasso and Random FOrest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0f1QrjwOC6X"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the three models for comparison\n",
        "plt.scatter(y_val, lasso_val_pred, alpha=0.5, label=f'Lasso (RMSE: 5.91)', color='blue')\n",
        "plt.scatter(y_val, rf_val_pred, alpha=0.5, label=f'RF (RMSE: 5.15)', color='green')\n",
        "plt.scatter(y_val, stack_val_pred, alpha=0.6, label=f'Stacked (RMSE: 5.04)', color='red', edgecolors='k')\n",
        "\n",
        "# Identity line (Perfect Prediction)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "\n",
        "plt.xlabel('Actual HDL-Cholesterol')\n",
        "plt.ylabel('Predicted HDL-Cholesterol')\n",
        "plt.title('Actual vs. Predicted: Base Models vs. Stacked Ensemble')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTj4QfX3iHGd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# 1. Base Model: Lasso (Scaled)\n",
        "plt.scatter(y_val, lasso_val_pred, alpha=0.4, label=f'Lasso Base (RMSE: 5.91)', color='#3498db')\n",
        "\n",
        "# 2. Base Model: Random Forest (Original)\n",
        "plt.scatter(y_val, rf_val_pred, alpha=0.4, label=f'RF Base (RMSE: 5.15)', color='#2ecc71')\n",
        "\n",
        "# 3. The \"Master\" Model: Stacked Ensemble\n",
        "plt.scatter(y_val, stack_val_pred, alpha=0.7, label=f'Stacked Ensemble (RMSE: 5.04)',\n",
        "            color='#e74c3c', edgecolors='white', linewidth=0.5)\n",
        "\n",
        "# Identity Line\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs. Predicted: How Stacking Combines Lasso & RF')\n",
        "plt.xlabel('Actual HDL-Cholesterol')\n",
        "plt.ylabel('Predicted HDL-Cholesterol')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=':', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¤ Final Predictions\n",
        "After identifying the **3-Stack** as our best-performing model (lowest Validation RMSE), we apply the same preprocessing pipeline to the test set and generate the final predictions for submission."
      ],
      "metadata": {
        "id": "Y5IrzXz2RZQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OAAcykxhyvv"
      },
      "outputs": [],
      "source": [
        "# 1. Load test data\n",
        "df_test = pd.read_csv(\"/test.csv\")\n",
        "\n",
        "# 2. Apply the same Human Readable labels used in training\n",
        "df_test.rename(columns=label_dict, inplace=True)\n",
        "\n",
        "# 3. Filter for ONLY the features your model was trained on\n",
        "# model_features = X_train.columns.tolist()\n",
        "X_test_final = df_test[X_train.columns]\n",
        "\n",
        "# 4. Handle any missing values in the test set (Random Forest requirement)\n",
        "X_test_final = X_test_final.fillna(X_train.median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wPByav0h29T"
      },
      "outputs": [],
      "source": [
        "# Generate predictions using the 3-model stack\n",
        "# Make sure 'stacking_model' is the variable name for your Lasso+RF+XGB stack\n",
        "test_predictions_triple = stacking_model.predict(X_test_final)\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': df_test.iloc[:, 0], # Grabs the unique ID from the first column\n",
        "    'LBDHDD_outcome': test_predictions_triple\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('pred.csv', index=False)\n",
        "print(\"Submission file 'pred.csv' is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjG8vkbmpCdo"
      },
      "source": [
        "Testing pred.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lrCP9FdpEeT"
      },
      "outputs": [],
      "source": [
        "# Load your newly created predictions\n",
        "preds_df = pd.read_csv(\"pred.csv\")\n",
        "\n",
        "print(\"--- Statistical Comparison ---\")\n",
        "print(f\"Train Actual Mean: {y_train.mean():.2f}\")\n",
        "print(f\"Test Prediction Mean: {preds_df['LBDHDD_outcome'].mean():.2f}\")\n",
        "print(\"\\n--- Value Range ---\")\n",
        "print(f\"Train Range: {y_train.min():.2f} to {y_train.max():.2f}\")\n",
        "print(f\"Test Range: {preds_df['LBDHDD_outcome'].min():.2f} to {preds_df['LBDHDD_outcome'].max():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rsSziwBpKLh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot Training Actuals\n",
        "sns.kdeplot(y_train, label='Train Actuals (Ground Truth)', color='blue', bw_adjust=1)\n",
        "\n",
        "# Plot Test Predictions\n",
        "sns.kdeplot(preds_df['LBDHDD_outcome'], label='Test Predictions (pred.csv)', color='red', bw_adjust=1)\n",
        "\n",
        "plt.title('Comparison of Predicted HDL Distribution vs. Training Ground Truth')\n",
        "plt.xlabel('HDL-Cholesterol Value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}